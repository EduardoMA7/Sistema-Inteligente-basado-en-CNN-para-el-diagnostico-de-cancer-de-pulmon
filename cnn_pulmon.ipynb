{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC09jqkuoRdjI8ajWsTUsc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoMA7/Sistema-Inteligente-basado-en-CNN-para-el-diagnostico-de-cancer-de-pulmon/blob/main/cnn_pulmon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Requerimientos**"
      ],
      "metadata": {
        "id": "zEEWKMLxnbad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YwcovfocUDdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2e4af8-86a8-4923-c054-471140425ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.46.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.46.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading streamlit-1.46.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=a306afdc81df5131a068e7074daede38b7a1751267d99a74e4f4827b34784ed4\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: kt-legacy, fpdf, watchdog, pyngrok, pydeck, keras-tuner, streamlit\n",
            "Successfully installed fpdf-1.7.2 keras-tuner-1.4.7 kt-legacy-1.0.5 pydeck-0.9.1 pyngrok-7.2.12 streamlit-1.46.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit tensorflow keras matplotlib seaborn pandas numpy scikit-learn pyngrok fpdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UZ_8R4SjcuSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e017df7a-4844-46c7-fbf6-6cfeddc4cd5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['NGROK_TOKEN'] = \"TOKEN\""
      ],
      "metadata": {
        "id": "M4-CmG_SCohM"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modulo de modelos (model_utils.py)**"
      ],
      "metadata": {
        "id": "gO14HWpdc1td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_utils.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
        "    Dense, Dropout, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import DenseNet121, ResNet50\n",
        "\n",
        "def load_models():\n",
        "    models = {}\n",
        "\n",
        "    # ---- Modelo 1: CNN 2D Personalizada (1 canal) ----\n",
        "    input_shape = (128, 128, 1)\n",
        "    inputs = Input(input_shape)\n",
        "    x = Conv2D(32, (3,3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(64, (3,3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv2D(128, (3,3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2,2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
        "                tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    models['CNN 2D Personalizada'] = model\n",
        "\n",
        "    # ---- Modelo 2: DenseNet121 (3 canales) ----\n",
        "    densenet_input = Input(shape=(128, 128, 3))\n",
        "    base_densenet = DenseNet121(weights='imagenet', include_top=False, input_tensor=densenet_input)\n",
        "    base_densenet.trainable = False\n",
        "\n",
        "    x = base_densenet.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    densenet_model = Model(inputs=densenet_input, outputs=output)\n",
        "    densenet_model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    models[\"DenseNet121\"] = densenet_model\n",
        "\n",
        "    # ---- Modelo 3: ResNet50 (3 canales) ----\n",
        "    resnet_input = Input(shape=(128, 128, 3))\n",
        "    base_resnet = ResNet50(weights='imagenet', include_top=False, input_tensor=resnet_input)\n",
        "    base_resnet.trainable = False\n",
        "\n",
        "    x = base_resnet.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(1, activation='sigmoid')(x)\n",
        "    resnet_model = Model(inputs=resnet_input, outputs=output)\n",
        "    resnet_model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "    models[\"ResNet50\"] = resnet_model\n",
        "\n",
        "    return models\n",
        "\n",
        "def predict_image(model, image):\n",
        "    if len(image.shape) == 2:\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    prediction = model.predict(image)\n",
        "    confidence = float(np.max(prediction))\n",
        "    diagnosis = 1 if prediction > 0.5 else 0\n",
        "    heatmap = generate_saliency_map(model, image[0])\n",
        "\n",
        "    return diagnosis, confidence, heatmap\n",
        "\n",
        "def predict_image_rgb(model, image_gray):\n",
        "    if len(image_gray.shape) == 2:\n",
        "        image_gray = np.expand_dims(image_gray, axis=-1)\n",
        "\n",
        "    image_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(image_gray)).numpy()\n",
        "    image_rgb_batch = np.expand_dims(image_rgb, axis=0)\n",
        "\n",
        "    prediction = model.predict(image_rgb_batch)\n",
        "    confidence = float(prediction[0][0])\n",
        "    diagnosis = 1 if confidence > 0.5 else 0\n",
        "    heatmap = generate_saliency_map(model, image_rgb)\n",
        "\n",
        "    return diagnosis, confidence, heatmap\n",
        "\n",
        "def generate_saliency_map(model, image):\n",
        "    if len(image.shape) == 3:\n",
        "        image_tensor = tf.convert_to_tensor(np.expand_dims(image, axis=0), dtype=tf.float32)\n",
        "    elif len(image.shape) == 4:\n",
        "        image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)\n",
        "    else:\n",
        "        raise ValueError(\"Imagen debe tener 3 (HWC) o 4 (BHWC) dimensiones.\")\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image_tensor)\n",
        "        prediction = model(image_tensor)\n",
        "\n",
        "    gradients = tape.gradient(prediction, image_tensor)\n",
        "    saliency_map = tf.reduce_max(tf.abs(gradients), axis=-1)[0].numpy()\n",
        "\n",
        "    if saliency_map.max() != saliency_map.min():\n",
        "        saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n",
        "    else:\n",
        "        saliency_map = np.zeros_like(saliency_map)\n",
        "\n",
        "    return saliency_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUsSx2e1JFbM",
        "outputId": "70fcd137-ed13-4332-a013-c2b7d5ac2f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M√≥dulo de Preprocesamiento (preprocessing.py)**"
      ],
      "metadata": {
        "id": "12rcHgyOdP2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocessing.py\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.exposure import equalize_adapthist\n",
        "\n",
        "def preprocess_image(image, target_size=(128, 128)):\n",
        "    image = resize(image, target_size, mode='reflect', anti_aliasing=True)\n",
        "    image = equalize_adapthist(image)\n",
        "    image = (image - image.min()) / (image.max() - image.min() + 1e-8)\n",
        "\n",
        "    return image\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
        "    image = imread(image_path, as_gray=True)\n",
        "    image_original = resize(image, target_size, mode='reflect', anti_aliasing=True)\n",
        "    image_preprocessed = preprocess_image(image, target_size)\n",
        "\n",
        "    return image_preprocessed, image_original"
      ],
      "metadata": {
        "id": "2GvwC9-ydtgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67e21095-4d49-4aa6-fe84-66995957aaf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importa dataset de drive -> Preprocesa -> Guarda en formato .npy\n",
        "import os\n",
        "import numpy as np\n",
        "from preprocessing import preprocess_image\n",
        "from skimage.io import imread\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess_and_save_dataset_npy(input_dir, output_dir, target_size=(128, 128)):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    classes = ['cancer', 'no_cancer']\n",
        "\n",
        "    for label in classes:\n",
        "        input_class_dir = os.path.join(input_dir, label)\n",
        "        output_class_dir = os.path.join(output_dir, label)\n",
        "        os.makedirs(output_class_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Procesando clase: {label} en {input_dir}\")\n",
        "        for file_name in tqdm(os.listdir(input_class_dir)):\n",
        "            input_path = os.path.join(input_class_dir, file_name)\n",
        "            file_base = os.path.splitext(file_name)[0]\n",
        "            output_path = os.path.join(output_class_dir, file_base + \".npy\")\n",
        "\n",
        "            try:\n",
        "                image = imread(input_path, as_gray=True)\n",
        "                processed_image = preprocess_image(image, target_size)\n",
        "                np.save(output_path, processed_image.astype(np.float32))\n",
        "            except Exception as e:\n",
        "                print(f\"Error con {file_name}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_input_dir = \"/content/drive/MyDrive/lung_cancer_MRI_dataset\"\n",
        "    base_output_dir = \"/content/processed_lung_dataset_npy\"\n",
        "\n",
        "    for subset in ['train', 'validate']:\n",
        "        input_dir = os.path.join(base_input_dir, subset)\n",
        "        output_dir = os.path.join(base_output_dir, subset)\n",
        "        preprocess_and_save_dataset_npy(input_dir, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC24ulZiXnJe",
        "outputId": "ec04a61c-a657-4902-bd4a-83705f22de0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando clase: cancer en /content/drive/MyDrive/lung_cancer_MRI_dataset/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1122/1122 [01:34<00:00, 11.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando clase: no_cancer en /content/drive/MyDrive/lung_cancer_MRI_dataset/train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1314/1314 [02:01<00:00, 10.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando clase: cancer en /content/drive/MyDrive/lung_cancer_MRI_dataset/validate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 752/752 [00:46<00:00, 16.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando clase: no_cancer en /content/drive/MyDrive/lung_cancer_MRI_dataset/validate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 492/492 [00:38<00:00, 12.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M√≥dulo de Entrenamiento (ml/train.py)**"
      ],
      "metadata": {
        "id": "YevFgxOcdRMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    EarlyStopping,\n",
        "    ReduceLROnPlateau,\n",
        "    TensorBoard\n",
        ")\n",
        "from model_utils import load_models\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.io import imread\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "\n",
        "def load_dataset(data_dir, test_size=0.2, random_state=42):\n",
        "    cancer_cases = [os.path.join(data_dir, 'cancer', f) for f in os.listdir(os.path.join(data_dir, 'cancer'))]\n",
        "    no_cancer_cases = [os.path.join(data_dir, 'no_cancer', f) for f in os.listdir(os.path.join(data_dir, 'no_cancer'))]\n",
        "\n",
        "    cancer_labels = [1] * len(cancer_cases)\n",
        "    no_cancer_labels = [0] * len(no_cancer_cases)\n",
        "\n",
        "    all_cases = cancer_cases + no_cancer_cases\n",
        "    all_labels = cancer_labels + no_cancer_labels\n",
        "\n",
        "    train_cases, val_cases, train_labels, val_labels = train_test_split(\n",
        "        all_cases, all_labels, test_size=test_size, random_state=random_state, stratify=all_labels\n",
        "    )\n",
        "\n",
        "    return train_cases, val_cases, train_labels, val_labels\n",
        "\n",
        "class LungImageGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, cases, labels, batch_size=32, image_size=(128, 128), augment=False, rgb=None, model_name=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.cases = cases\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "        self.model_name = model_name\n",
        "        self.indices = np.arange(len(self.cases))\n",
        "\n",
        "        if rgb is not None:\n",
        "            self.rgb = rgb\n",
        "        elif self.model_name in [\"DenseNet121\", \"ResNet50\"]:\n",
        "            self.rgb = True\n",
        "        else:\n",
        "            self.rgb = False\n",
        "\n",
        "        if self.augment:\n",
        "            self.augmenter = ImageDataGenerator(\n",
        "                rotation_range=10,\n",
        "                width_shift_range=0.05,\n",
        "                height_shift_range=0.05,\n",
        "                zoom_range=0.1,\n",
        "                horizontal_flip=True,\n",
        "                fill_mode='reflect'\n",
        "            )\n",
        "        else:\n",
        "            self.augmenter = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.cases) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        batch_cases = [self.cases[i] for i in batch_indices]\n",
        "        batch_labels = [self.labels[i] for i in batch_indices]\n",
        "\n",
        "        batch_images = []\n",
        "        for path in batch_cases:\n",
        "          #image = imread(path, as_gray=True)\n",
        "          image = np.load(path).astype(np.float32)\n",
        "          if len(image.shape) == 2:\n",
        "            image = np.expand_dims(image, axis=-1)\n",
        "\n",
        "          if self.rgb:\n",
        "            image = tf.image.grayscale_to_rgb(tf.convert_to_tensor(image)).numpy()\n",
        "            #image = image.astype('float32') / 255.0\n",
        "\n",
        "          batch_images.append(image)\n",
        "\n",
        "        batch_images = np.array(batch_images, dtype=np.float32)\n",
        "        batch_labels = np.array(batch_labels, dtype=np.float32)\n",
        "\n",
        "        if self.augment:\n",
        "            aug_iter = self.augmenter.flow(batch_images, batch_labels, batch_size=self.batch_size, shuffle=False)\n",
        "            batch_images, batch_labels = next(aug_iter)\n",
        "\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "def train_model(model, train_generator, val_generator, epochs, model_name='lung_cancer_model', class_weight=None):\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(\n",
        "            f'models/{model_name}.keras',\n",
        "            monitor='val_auc',\n",
        "            save_best_only=True,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "        TensorBoard(\n",
        "            log_dir=f'logs/{model_name}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}',\n",
        "            histogram_freq=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weight,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    pd.DataFrame(history.history).to_csv(f'models/{model_name}_history.csv', index=False)\n",
        "\n",
        "    return history\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_dir = '/content/processed_lung_dataset_npy/train'\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "    train_cases, val_cases, train_labels, val_labels = load_dataset(data_dir)\n",
        "\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(train_labels),\n",
        "        y=train_labels\n",
        "    )\n",
        "    class_weight = dict(enumerate(class_weights))\n",
        "\n",
        "    models = load_models()\n",
        "    modelo_objetivo = \"DenseNet121\"\n",
        "    models = {k: v for k, v in models.items() if k == modelo_objetivo}\n",
        "\n",
        "    train_gen = LungImageGenerator(train_cases, train_labels, batch_size=32, augment=True, model_name=modelo_objetivo)\n",
        "    val_gen = LungImageGenerator(val_cases, val_labels, batch_size=32, augment=False, model_name=modelo_objetivo)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nEntrenando modelo: {name}\")\n",
        "        history = train_model(\n",
        "            model,\n",
        "            train_gen,\n",
        "            val_gen,\n",
        "            epochs=50,\n",
        "            model_name=f\"{name.lower().replace(' ', '_')}_lung_cancer\",\n",
        "            class_weight=class_weight\n",
        "        )"
      ],
      "metadata": {
        "id": "gurjOLtWdt6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715a7144-6f78-4868-f3f8-e3ac84878873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aplicaci√≥n Principal (app.py)**\n"
      ],
      "metadata": {
        "id": "ukQTz4qrdkvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from model_utils import predict_image, predict_image_rgb\n",
        "from preprocessing import load_and_preprocess_image\n",
        "from report_utils import generate_pdf_report\n",
        "from tensorflow.keras.models import load_model\n",
        "from report_utils import generate_pdf_report, generate_comparison_report\n",
        "from metrics_utils import evaluate_on_dataset\n",
        "import time\n",
        "import glob\n",
        "import json\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Sistema de Diagn√≥stico de C√°ncer de Pulm√≥n\",\n",
        "    page_icon=\"üè•\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "MODEL_PATHS = {\n",
        "    \"CNN 2D Personalizada\": \"/content/models/cnn_2d_personalizada_lung_cancer.keras\",\n",
        "    \"DenseNet121\": \"/content/models/densenet121_lung_cancer.keras\",\n",
        "    \"ResNet50\": \"/content/models/resnet50_lung_cancer.keras\"\n",
        "}\n",
        "DATASET_PATH = \"/content/drive/MyDrive/lung_cancer_MRI_dataset/validate\"\n",
        "JSON_DIR = \"/content/resultados/json\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_selected_model(model_name):\n",
        "    model_path = MODEL_PATHS.get(model_name)\n",
        "    if model_path and os.path.exists(model_path):\n",
        "        return load_model(model_path)\n",
        "    else:\n",
        "        st.error(f\"No se encontr√≥ el modelo: {model_path}\")\n",
        "        st.stop()\n",
        "\n",
        "def load_saved_results():\n",
        "    try:\n",
        "        metrics_path = os.path.join(JSON_DIR, \"metrics.json\")\n",
        "        comparisons_path = os.path.join(JSON_DIR, \"comparisons.json\")\n",
        "        image_paths_path = os.path.join(JSON_DIR, \"image_paths.json\")\n",
        "\n",
        "        if not all(os.path.exists(p) for p in [metrics_path, comparisons_path, image_paths_path]):\n",
        "            return None\n",
        "\n",
        "        with open(metrics_path, \"r\") as f:\n",
        "            metrics_list = json.load(f)\n",
        "\n",
        "        with open(comparisons_path, \"r\") as f:\n",
        "            comparisons_raw = json.load(f)\n",
        "        comparisons = {k: tuple(v) for k, v in comparisons_raw.items()}\n",
        "\n",
        "        with open(image_paths_path, \"r\") as f:\n",
        "            paths = json.load(f)\n",
        "\n",
        "        return metrics_list, comparisons, paths[\"confusions\"], paths[\"rocs\"], paths[\"prs\"]\n",
        "\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def mostrar_resultados(metrics_list, comparisons, confusion_matrices, roc_paths, pr_paths):\n",
        "    st.subheader(\"üìä M√©tricas por Modelo\")\n",
        "    for metrics in metrics_list:\n",
        "        st.markdown(f\"### {metrics['model']}\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        with col1:\n",
        "            st.metric(\"Precisi√≥n\", f\"{metrics['accuracy']:.3f}\")\n",
        "            st.metric(\"F1-Score\", f\"{metrics['f1']:.3f}\")\n",
        "        with col2:\n",
        "            st.metric(\"Sensibilidad\", f\"{metrics['sensitivity']:.3f}\")\n",
        "            st.metric(\"Especificidad\", f\"{metrics['specificity']:.3f}\")\n",
        "        with col3:\n",
        "            st.metric(\"MCC\", f\"{metrics['mcc']:.3f}\")\n",
        "\n",
        "    st.subheader(\"üìà Comparaciones Estad√≠sticas (McNemar)\")\n",
        "    for key, (stat, p_value) in comparisons.items():\n",
        "        m1, m2 = key.split('_')\n",
        "        name1 = [\"CNN 2D Personalizada\", \"DenseNet121\", \"ResNet50\"][int(m1)]\n",
        "        name2 = [\"CNN 2D Personalizada\", \"DenseNet121\", \"ResNet50\"][int(m2)]\n",
        "        st.write(f\"**{name1} vs {name2}** ‚Äî Estad√≠stico: `{stat:.4f}` | p-valor: `{p_value:.4f}`\")\n",
        "\n",
        "    st.subheader(\"üßÆ Matrices de Confusi√≥n\")\n",
        "    cols = st.columns(2)\n",
        "    for i, cm_path in enumerate(confusion_matrices):\n",
        "        with cols[i % 2]:\n",
        "            st.image(cm_path, caption=f\"Matriz de Confusi√≥n - {['CNN 2D Personalizada', 'DenseNet121', 'ResNet50'][i]}\", use_container_width=True)\n",
        "\n",
        "    st.subheader(\"üìâ Curvas ROC y Precision-Recall\")\n",
        "    for i, model_name in enumerate([\"CNN 2D Personalizada\", \"DenseNet121\", \"ResNet50\"]):\n",
        "        st.markdown(f\"### {model_name}\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.image(roc_paths[i], caption=\"Curva ROC\", use_container_width=True)\n",
        "        with col2:\n",
        "            st.image(pr_paths[i], caption=\"Curva Precision-Recall\", use_container_width=True)\n",
        "\n",
        "st.sidebar.title(\"Navegaci√≥n\")\n",
        "page = st.sidebar.radio(\"Ir a\", [\"Diagn√≥stico por Imagen\", \"Evaluaci√≥n de Modelos\"])\n",
        "\n",
        "if page == \"Diagn√≥stico por Imagen\":\n",
        "    st.title(\"Sistema Inteligente de Diagn√≥stico de C√°ncer de Pulm√≥n\")\n",
        "    st.markdown(\"\"\"\n",
        "    Este sistema utiliza redes neuronales convolucionales para analizar radiograf√≠as o cortes individuales de tomograf√≠as\n",
        "    en formato PNG y detectar signos tempranos de c√°ncer pulmonar.\n",
        "    \"\"\")\n",
        "\n",
        "    model_names = list(MODEL_PATHS.keys())\n",
        "    selected_model_name = st.sidebar.selectbox(\"Modelo a utilizar\", model_names)\n",
        "\n",
        "    confidence_threshold = st.sidebar.slider(\n",
        "        \"Umbral de confianza para diagn√≥stico\",\n",
        "        min_value=0.1, max_value=0.99, value=0.5, step=0.01\n",
        "    )\n",
        "\n",
        "    model = load_selected_model(selected_model_name)\n",
        "\n",
        "    st.header(\"Carga de Imagen\")\n",
        "    upload_option = st.radio(\"Seleccione el tipo de entrada\", [\"Subir imagen PNG\", \"Usar ejemplo\"])\n",
        "\n",
        "    uploaded_file = None\n",
        "\n",
        "    if upload_option == \"Subir imagen PNG\":\n",
        "        uploaded_file = st.file_uploader(\"Suba una imagen pulmonar en formato PNG\", type=[\"png\"])\n",
        "    else:\n",
        "        class_folders = [f for f in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, f))]\n",
        "        selected_class = st.selectbox(\"Seleccione clase de ejemplo\", class_folders)\n",
        "        selected_folder = os.path.join(DATASET_PATH, selected_class)\n",
        "        class_images = glob.glob(os.path.join(selected_folder, \"*.png\"))\n",
        "\n",
        "        if class_images:\n",
        "            file_names = [os.path.basename(f) for f in class_images]\n",
        "            file_choice = st.selectbox(\"Seleccione imagen\", file_names)\n",
        "            if file_choice:\n",
        "                uploaded_file = os.path.join(selected_folder, file_choice)\n",
        "            else:\n",
        "                st.warning(\"Seleccione una imagen v√°lida.\")\n",
        "        else:\n",
        "            st.warning(f\"No se encontraron im√°genes en la carpeta '{selected_class}'.\")\n",
        "\n",
        "    if uploaded_file:\n",
        "        with st.spinner(\"Procesando imagen...\"):\n",
        "            image_preprocessed, image_original = load_and_preprocess_image(uploaded_file)\n",
        "            time.sleep(1)\n",
        "\n",
        "        st.subheader(\"Visualizaci√≥n de Imagen\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.image(image_original, caption=\"Imagen Original Redimensionada\", use_container_width=True)\n",
        "        with col2:\n",
        "            st.image(image_preprocessed, caption=\"Imagen Preprocesada\", use_container_width=True)\n",
        "\n",
        "        st.header(\"Resultados del Diagn√≥stico\")\n",
        "        with st.spinner(\"Analizando imagen...\"):\n",
        "            if selected_model_name in [\"ResNet50\", \"DenseNet121\"]:\n",
        "                prediction, confidence, heatmap = predict_image_rgb(model, image_preprocessed)\n",
        "            else:\n",
        "                prediction, confidence, heatmap = predict_image(model, image_preprocessed)\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        with col1:\n",
        "            st.metric(\"Modelo utilizado\", selected_model_name)\n",
        "        with col2:\n",
        "            st.metric(\"Predicci√≥n\", \"Positivo para c√°ncer\" if prediction == 1 else \"Negativo para c√°ncer\")\n",
        "        with col3:\n",
        "            st.metric(\"Confianza\", f\"{confidence:.2%}\")\n",
        "\n",
        "        if confidence < confidence_threshold:\n",
        "            st.warning(\"La confianza es baja. Se recomienda evaluaci√≥n adicional.\")\n",
        "        else:\n",
        "            if prediction == 0:\n",
        "                st.success(\"No se detectaron signos malignos. Seguimiento rutinario recomendado.\")\n",
        "            else:\n",
        "                st.error(\"Posibles signos de c√°ncer detectados. Consulte a un especialista.\")\n",
        "\n",
        "        st.subheader(\"Mapa de Saliencia (Regiones relevantes)\")\n",
        "        fig, ax = plt.subplots(figsize=(6,6))\n",
        "        ax.imshow(image_original, cmap='gray')\n",
        "        ax.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "        ax.axis('off')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.subheader(\"Generar Reporte PDF\")\n",
        "        if st.button(\"üìÑ Descargar Reporte PDF\"):\n",
        "            generate_pdf_report(\n",
        "                image_original=image_original,\n",
        "                heatmap=heatmap,\n",
        "                diagnosis=prediction,\n",
        "                confidence=confidence,\n",
        "                model_name=selected_model_name\n",
        "            )\n",
        "\n",
        "elif page == \"Evaluaci√≥n de Modelos\":\n",
        "    st.title(\"Evaluaci√≥n Comparativa de Modelos\")\n",
        "    st.markdown(\"Esta secci√≥n permite evaluar los modelos con el dataset de validaci√≥n y comparar sus m√©tricas.\")\n",
        "\n",
        "    loaded_results = load_saved_results()\n",
        "\n",
        "    if loaded_results:\n",
        "        st.info(\"Mostrando resultados previamente evaluados.\")\n",
        "        metrics_list, comparisons, confusion_matrices, roc_paths, pr_paths = loaded_results\n",
        "        mostrar_resultados(*loaded_results)\n",
        "\n",
        "    st.divider()\n",
        "    st.markdown(\"### ¬øDeseas volver a evaluar los modelos?\")\n",
        "    if st.button(\"üîç Evaluar Modelos y Generar Reporte Comparativo\"):\n",
        "        with st.spinner(\"Evaluando modelos...\"):\n",
        "            test_dir = \"/content/processed_lung_dataset_npy/validate\"\n",
        "            metrics_list, comparisons, confusion_matrices, roc_paths, pr_paths = evaluate_on_dataset(test_dir)\n",
        "            report_path = generate_comparison_report(\n",
        "                metrics_list,\n",
        "                [\"CNN 2D Personalizada\", \"DenseNet121\", \"ResNet50\"],\n",
        "                comparisons,\n",
        "                confusion_matrices\n",
        "            )\n",
        "\n",
        "        with open(report_path, \"rb\") as f:\n",
        "            st.download_button(\"üì• Descargar Reporte Comparativo\", data=f, file_name=\"comparacion_modelos.pdf\", mime=\"application/pdf\")\n",
        "        st.success(\"¬°Evaluaci√≥n completada y reporte generado con √©xito!\")\n",
        "\n",
        "        mostrar_resultados(metrics_list, comparisons, confusion_matrices, roc_paths, pr_paths)"
      ],
      "metadata": {
        "id": "Zcc7ySCiduzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5fba8b-da21-46c7-aecb-a34689c40f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "conf.get_default().auth_token = os.getenv(\"NGROK_TOKEN\")\n",
        "\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(f\"Tu app est√° en: {public_url}\")\n",
        "\n",
        "!streamlit run app.py"
      ],
      "metadata": {
        "id": "sm48TTpgeSZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8cbfda8-01b4-4e76-d2ba-8329e72dbe1b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tu app est√° en: NgrokTunnel: \"https://98faa725256d.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.72.211.175:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **M√©tricas (metrics_utils.py)**"
      ],
      "metadata": {
        "id": "OX7Yjem7KqXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile metrics_utils.py\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from scipy.stats import chi2\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "RESULTADOS_DIR = \"/content/resultados\"\n",
        "os.makedirs(RESULTADOS_DIR, exist_ok=True)\n",
        "JSON_DIR = os.path.join(RESULTADOS_DIR, \"json\")\n",
        "os.makedirs(JSON_DIR, exist_ok=True)\n",
        "\n",
        "def matthews_corrcoef(cm):\n",
        "    tp, fp, fn, tn = cm[1][1], cm[0][1], cm[1][0], cm[0][0]\n",
        "    numerator = (tp * tn) - (fp * fn)\n",
        "    denominator = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
        "    return numerator / denominator if denominator != 0 else 0\n",
        "\n",
        "def mcnemar_test(y_true, y_model1, y_model2):\n",
        "    table = np.zeros((2, 2))\n",
        "    for true, pred1, pred2 in zip(y_true, y_model1, y_model2):\n",
        "        if pred1 == true and pred2 != true:\n",
        "            table[0][1] += 1\n",
        "        elif pred1 != true and pred2 == true:\n",
        "            table[1][0] += 1\n",
        "    if table[0][1] + table[1][0] > 25:\n",
        "        statistic = (np.abs(table[0][1] - table[1][0]) - 1) ** 2 / (table[0][1] + table[1][0])\n",
        "    else:\n",
        "        statistic = (np.abs(table[0][1] - table[1][0])) ** 2 / (table[0][1] + table[1][0])\n",
        "    p_value = 1 - chi2.cdf(statistic, df=1)\n",
        "    return statistic, p_value\n",
        "\n",
        "def save_curves(model_name, y_true, y_scores, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model_safe = model_name.lower().replace(\" \", \"_\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title(f'Curva ROC - {model_name}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    roc_path = os.path.join(output_dir, f'roc_{model_safe}.png')\n",
        "    plt.savefig(roc_path)\n",
        "    plt.close()\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "    ap_score = average_precision_score(y_true, y_scores)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(recall, precision, label=f'AP = {ap_score:.2f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precisi√≥n')\n",
        "    plt.title(f'Curva Precision-Recall - {model_name}')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    pr_path = os.path.join(output_dir, f'pr_{model_safe}.png')\n",
        "    plt.savefig(pr_path)\n",
        "    plt.close()\n",
        "\n",
        "    return roc_path, pr_path\n",
        "\n",
        "def evaluate_on_dataset(test_dir):\n",
        "    def load_data():\n",
        "        cases, labels = [], []\n",
        "        for label_name, label in [('no_cancer', 0), ('cancer', 1)]:\n",
        "            folder = os.path.join(test_dir, label_name)\n",
        "            for img_file in os.listdir(folder):\n",
        "                img = np.load(os.path.join(folder, img_file))\n",
        "                if len(img.shape) == 2:\n",
        "                    img = np.expand_dims(img, axis=-1)\n",
        "                cases.append(img)\n",
        "                labels.append(label)\n",
        "        return np.array(cases), np.array(labels)\n",
        "\n",
        "    X, y_true = load_data()\n",
        "\n",
        "    models = {\n",
        "        \"CNN 2D Personalizada\": load_model(\"/content/models/cnn_2d_personalizada_lung_cancer.keras\"),\n",
        "        \"DenseNet121\": load_model(\"/content/models/densenet121_lung_cancer.keras\"),\n",
        "        \"ResNet50\": load_model(\"/content/models/resnet50_lung_cancer.keras\")\n",
        "    }\n",
        "\n",
        "    metrics_list = []\n",
        "    predictions_per_model = []\n",
        "    confusion_matrices = []\n",
        "    roc_paths = []\n",
        "    pr_paths = []\n",
        "\n",
        "    for name, model in models.items():\n",
        "        if model.input_shape[-1] == 3:\n",
        "            X_processed = np.repeat(X, 3, axis=-1)\n",
        "        else:\n",
        "            X_processed = X\n",
        "\n",
        "        preds = model.predict(X_processed, batch_size=32).flatten()\n",
        "        y_pred = (preds > 0.5).astype(int)\n",
        "        predictions_per_model.append(y_pred)\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        report = classification_report(y_true, y_pred, target_names=['no_cancer', 'cancer'], output_dict=True)\n",
        "\n",
        "        metrics_list.append({\n",
        "            'model': name,\n",
        "            'accuracy': report['accuracy'],\n",
        "            'sensitivity': report['cancer']['recall'],\n",
        "            'specificity': report['no_cancer']['recall'],\n",
        "            'f1': report['cancer']['f1-score'],\n",
        "            'mcc': matthews_corrcoef(cm)\n",
        "        })\n",
        "\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                    xticklabels=['No C√°ncer', 'C√°ncer'],\n",
        "                    yticklabels=['No C√°ncer', 'C√°ncer'])\n",
        "        plt.title(f'Matriz de Confusi√≥n - {name}')\n",
        "        plt.xlabel('Predicho')\n",
        "        plt.ylabel('Real')\n",
        "\n",
        "        cm_path = os.path.join(RESULTADOS_DIR, f'cm_{name.replace(\" \", \"_\").lower()}.png')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(cm_path)\n",
        "        plt.close()\n",
        "\n",
        "        confusion_matrices.append(cm_path)\n",
        "\n",
        "        roc_path, pr_path = save_curves(name, y_true, preds, RESULTADOS_DIR)\n",
        "        roc_paths.append(roc_path)\n",
        "        pr_paths.append(pr_path)\n",
        "\n",
        "    comparisons = {}\n",
        "    comparisons['0_1'] = mcnemar_test(y_true, predictions_per_model[0], predictions_per_model[1])\n",
        "    comparisons['0_2'] = mcnemar_test(y_true, predictions_per_model[0], predictions_per_model[2])\n",
        "    comparisons['1_2'] = mcnemar_test(y_true, predictions_per_model[1], predictions_per_model[2])\n",
        "\n",
        "    metrics_json_path = os.path.join(JSON_DIR, \"metrics.json\")\n",
        "    with open(metrics_json_path, \"w\") as f:\n",
        "        json.dump(metrics_list, f, indent=4)\n",
        "\n",
        "    comparisons_json_path = os.path.join(JSON_DIR, \"comparisons.json\")\n",
        "    comparisons_str = {k: [float(v[0]), float(v[1])] for k, v in comparisons.items()}\n",
        "    with open(comparisons_json_path, \"w\") as f:\n",
        "        json.dump(comparisons_str, f, indent=4)\n",
        "\n",
        "    paths_json_path = os.path.join(JSON_DIR, \"image_paths.json\")\n",
        "    paths_data = {\n",
        "        \"confusions\": confusion_matrices,\n",
        "        \"rocs\": roc_paths,\n",
        "        \"prs\": pr_paths\n",
        "    }\n",
        "    with open(paths_json_path, \"w\") as f:\n",
        "        json.dump(paths_data, f, indent=4)\n",
        "\n",
        "    return metrics_list, comparisons, confusion_matrices, roc_paths, pr_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ismHPGV3Kqiu",
        "outputId": "520f7ee8-b784-4e8d-cbb5-c7ddafd8b698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting metrics_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generar PDF (report_utils.py)**"
      ],
      "metadata": {
        "id": "cdEGMpCR5s-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile report_utils.py\n",
        "from fpdf import FPDF\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import streamlit as st\n",
        "import time\n",
        "\n",
        "RESULTADOS_DIR = \"/content/resultados\"\n",
        "os.makedirs(RESULTADOS_DIR, exist_ok=True)\n",
        "\n",
        "def generate_pdf_report(image_original, heatmap, diagnosis, confidence, model_name):\n",
        "    combined_path = os.path.join(RESULTADOS_DIR, \"combined_temp.png\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "    axes[0].imshow(image_original, cmap='gray')\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(image_original, cmap='gray')\n",
        "    axes[1].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    axes[1].set_title(\"Mapa de Saliencia\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(combined_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    pdf.cell(200, 10, txt=\"Reporte de Diagn√≥stico Pulmonar\", ln=1, align='C')\n",
        "    pdf.ln(5)\n",
        "\n",
        "    pdf.cell(200, 10, txt=f\"Modelo utilizado: {model_name}\", ln=1)\n",
        "    pdf.cell(200, 10, txt=f\"Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\", ln=1)\n",
        "    pdf.cell(200, 10, txt=f\"Diagn√≥stico: {'Positivo para C√°ncer' if diagnosis else 'Negativo para C√°ncer'}\", ln=1)\n",
        "    pdf.cell(200, 10, txt=f\"Confianza: {confidence*100:.2f}%\", ln=1)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    pdf.cell(200, 10, txt=\"Im√°genes de Diagn√≥stico:\", ln=1)\n",
        "    pdf.image(combined_path, w=180)\n",
        "\n",
        "    pdf_path = os.path.join(RESULTADOS_DIR, \"diagnostico_pulmonar.pdf\")\n",
        "    pdf.output(pdf_path)\n",
        "\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        st.download_button(\"üì• Descargar Reporte de Diagn√≥stico\", data=f, file_name=pdf_path, mime=\"application/pdf\")\n",
        "\n",
        "    os.remove(combined_path)\n",
        "    os.remove(pdf_path)\n",
        "\n",
        "def generate_comparison_report(metrics_list, model_names, comparisons, confusion_matrices, output_path=os.path.join(RESULTADOS_DIR, 'report_comparativo.pdf')):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    pdf.cell(200, 10, txt=\"Reporte Comparativo de Modelos\", ln=1, align='C')\n",
        "    pdf.cell(200, 10, txt=f\"Fecha: {time.strftime('%Y-%m-%d %H:%M:%S')}\", ln=1)\n",
        "    pdf.ln(5)\n",
        "\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"M√©tricas Generales\", ln=1)\n",
        "    pdf.set_font(\"Arial\", 'B', 10)\n",
        "\n",
        "    col_width = 32\n",
        "    headers = [\"Modelo\", \"Precisi√≥n\", \"Sensibilidad\", \"Especificidad\", \"F1-Score\", \"MCC\"]\n",
        "\n",
        "    for header in headers:\n",
        "        pdf.cell(col_width, 8, header, border=1, align='C')\n",
        "    pdf.ln()\n",
        "\n",
        "    pdf.set_font(\"Arial\", size=10)\n",
        "    for i, model_name in enumerate(model_names):\n",
        "        metrics = metrics_list[i]\n",
        "        pdf.cell(col_width, 8, model_name, border=1)\n",
        "        pdf.cell(col_width, 8, f\"{metrics['accuracy']:.3f}\", border=1, align='C')\n",
        "        pdf.cell(col_width, 8, f\"{metrics['sensitivity']:.3f}\", border=1, align='C')\n",
        "        pdf.cell(col_width, 8, f\"{metrics['specificity']:.3f}\", border=1, align='C')\n",
        "        pdf.cell(col_width, 8, f\"{metrics['f1']:.3f}\", border=1, align='C')\n",
        "        pdf.cell(col_width, 8, f\"{metrics['mcc']:.3f}\", border=1, align='C')\n",
        "        pdf.ln()\n",
        "\n",
        "    pdf.ln(5)\n",
        "\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(200, 10, txt=\"Comparaciones Estad√≠sticas (McNemar)\", ln=1)\n",
        "    pdf.set_font(\"Arial\", size=10)\n",
        "\n",
        "    for key, (stat, p_value) in comparisons.items():\n",
        "        m1, m2 = key.split('_')\n",
        "        pdf.multi_cell(0, 8, txt=(\n",
        "            f\"Comparaci√≥n {model_names[int(m1)]} vs {model_names[int(m2)]}:\\n\"\n",
        "            f\"Estad√≠stico de McNemar = {stat:.4f}, p-value = {p_value:.4f}\\n\"\n",
        "        ))\n",
        "\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", 'B', 12)\n",
        "    pdf.cell(0, 10, \"Matrices de Confusi√≥n\", ln=1)\n",
        "    pdf.set_font(\"Arial\", size=10)\n",
        "    pdf.ln(5)\n",
        "\n",
        "    x_positions = [10, 110]\n",
        "    y_position = pdf.get_y()\n",
        "\n",
        "    for idx, cm_path in enumerate(confusion_matrices):\n",
        "        col = idx % 2\n",
        "        if idx != 0 and col == 0:\n",
        "            y_position += 80\n",
        "\n",
        "        pdf.set_xy(x_positions[col], y_position)\n",
        "        pdf.cell(90, 10, txt=f\"Matriz - {model_names[idx]}\", ln=2)\n",
        "\n",
        "        pdf.set_x(x_positions[col])\n",
        "        pdf.image(cm_path, x=x_positions[col], y=pdf.get_y(), w=80)\n",
        "\n",
        "    pdf.output(output_path)\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMSWdYZwkpe3",
        "outputId": "93d9282b-5e17-448a-ffe8-efd79ae79ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting report_utils.py\n"
          ]
        }
      ]
    }
  ]
}